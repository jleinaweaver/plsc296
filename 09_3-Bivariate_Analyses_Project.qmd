---
format: revealjs
---

# Today's Agenda {background-image="Images/background-data_blue_v4.png"}

```{r}
library(tidyverse)
library(readxl)
library(kableExtra)

# Input merged project data
d <- read_excel("../Data_in_Class/Merged_Dataset-2006-2016.xlsx", na = "NA")
```

<br>

<br>

**Apply bivariate analyses to our research project**

<br>

<br>

::: r-stack
Justin Leinaweaver (Spring 2025)
:::

::: notes
Prep for Class

1. Make the merged data file available on Canvas
:::



## Present your "best" visualizations {background-image="Images/background-slate_v2.png" .center}

<br>

1. 'cut' and 'clarity'

2. 'cut' and 'price'

3. 'carat' and 'price'

<br>

**Source:** 'diamonds' dataset in tidyverse

::: notes
Ok, here's what I asked you to do for class today

- My hope is that this gave you a chance to practice all of the visualizations from last class, PLUS

- A chance to practice our polishing skills again
:::



## How did you visualize the relationship between `cut` and `clarity`? {background-image="Images/background-slate_v2.png" .center}

```{r, fig.align='center'}
knitr::include_graphics("Images/09_3-Diamond_Cut.png")
```

```{r, fig.align='center'}
knitr::include_graphics("Images/09_3-Diamond_Clarity.jpg")
```

::: notes
Alright, let's see what everybody made!

- *Encourage everyone to walk around and see all the plots*

<br>

**What are we seeing? What do we like?**

<br>

**SLIDE**: Here's my stab
:::


## {background-image="Images/background-slate_v2.png" .center .smaller}

```{r, echo=TRUE, fig.align='center', fig.asp=.618, fig.width=8, cache=TRUE}
ggplot(diamonds, aes(x = clarity, fill = forcats::fct_rev(cut))) +
  geom_bar(position = "fill") +
  theme_bw() +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_brewer(type = "seq", palette = 4) +
  labs(x = "Diamond Clarity", fill = "Cut Type",
       caption = "Source: The Loose Diamonds Search Engine",
       title = "Jewelers appear to use greater care when shaping higher quality diamonds")
```



## How did you visualize the relationship between `cut` and `price`? {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center'}
knitr::include_graphics("Images/09_3-Diamond_Cut2.webp")
```

::: notes
Alright, let's see what everybody made!

- *Encourage everyone to walk around and see all the plots*

<br>

**What are we seeing? What do we like?**

<br>

**SLIDE**: Here's my stab
:::



## Diamonds: 'cut' and 'price' {background-image="Images/background-slate_v2.png" .center}

:::: {.columns}
::: {.column width="45%"}

<br>

```{r, echo=FALSE, fig.align='center', fig.asp=1, fig.width=6, cache=TRUE}
ggplot(diamonds, aes(x = price, y = forcats::fct_rev(cut))) +
  geom_boxplot(fill = "seagreen2") +
  theme_bw() +
  labs(x = "", y = "", title = "Does the quality of the a diamond's cut influence its price?", caption = "Source: The Loose Diamonds Search Engine") +
  scale_x_continuous(labels = scales::dollar_format()) 
```
:::

::: {.column width="55%"}
```{r, echo=FALSE, fig.align='center', fig.asp=1.1, fig.width=7, cache=TRUE}
ggplot(diamonds, aes(x = price)) +
  geom_histogram(bins = 15) +
  facet_wrap(~ cut, ncol = 1) +
  theme_bw() +
  scale_x_continuous(labels = scales::dollar_format()) 
```
:::
::::

::: notes
The distributions overlap considerably.

- Doesn't look like cut and price are highly correlated!

<br>

**Any guesses why cut and price appear uncorrelated here?**

- (Size, e.g. carats, matters in diamonds a ton!)

<br>

**SLIDE**: Last one focuses on size!
:::



## How did you visualize the relationship between `carat` and `price`? {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center'}
knitr::include_graphics("Images/09_3-Diamond_Carats.jpg")
```

::: notes
Alright, let's see what everybody made!

- *Encourage everyone to walk around and see all the plots*

<br>

**What are we seeing? What do we like?**

<br>

**SLIDE**: Here's my stab
:::



## Diamonds: 'carat' and 'price' {background-image="Images/background-slate_v2.png" .center .smaller}

```{r, echo=TRUE, fig.align='center', fig.width=9, fig.asp=.7, cache=TRUE}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(alpha = .05) +
  theme_bw() +
  labs(x = "Size of the Diamond (carats)", y = "Prices",
       title = "The larger the diamond the more expensive!") +
  scale_y_continuous(labels = scales::dollar_format())
```

::: notes
The alpha argument in geom_point is new for our class pictures

- Given the 50k diamonds in the dataset it can be VERY hard to see through the cloud of points

- The alpha changes the opacity of the points, e.g. how dark each one is

- I've set the alpha SUPER low so we can try to see somewhat through the cloud

<br>

**Does anything interesting stand out in this plot thanks to the very low alpha setting?**

- The dark vertical bands at round-ish numbers of carats (1, 1.25, 1.5, 1.75, 2) implies jewelers are actively making cuts in order to create tiers in the market

- If they cut what they got in order to preserve the original size everytime we'd see much more variation

- The diamond market is ten layers of nonsense and marketing
:::



## Bivariate Analyses {background-image="Images/background-slate_v2.png" .center}

<br>

1. Descriptive Statistics by Group
    - Counts, Proportions, Means, Medians, Standard Deviations, Percentiles, Ranges, Interquartile Ranges (IQR) and Correlations

2. Visualizations
    - Bar plots (stacked, side-by-side, proportions), box plots and scatter plots

::: notes
**Any questions on our bivariate analyses tools?**

<br>

My expectation from here on out is that you can use all of these tools in R
:::



## Are powerful states more likely to respect the human rights of their citizens? {background-image="Images/background-slate_v2.png" .center .smaller}

:::: {.columns}
::: {.column width="20%"}
CoW NMC Dataset

- `cinc`
- `milex`
- `milper`
- `irst`
- `pec`
- `tpop`
- `upop`
:::

::: {.column width="60%"}

<br>

<br>

```{r, fig.width = 9, fig.height=2}
## Manual DAG
d1 <- tibble(
  x = c(-3, 3),
  y = c(1, 1),
  labels = c("Military\nPower\n(CoW NMC)", "Human rights\nand\nthe rule of law\n(FFP)")
)

ggplot(data = d1, aes(x = x, y = y)) +
  geom_point(size = 8) +
  theme_void() +
  coord_cartesian(xlim = c(-4, 4)) +
  geom_label(aes(label = labels), size = 7) +
  annotate("segment", x = -1.9, xend = 1.85, y = 1, yend = 1, arrow = arrow())
```
:::

::: {.column width="20%"}
Fund for Peace Dataset

- `P3_Human_Rts`
:::
::::

::: notes
Here's where I believe we are currently in developing our research project.

- It's now time for us to start testing the mechanism

- Is there evidence of an association in the data between state material capabilities and a respect for human rights / the rule of law?
:::




## Make a scatterplot and calculate the correlation for each pairing in 2016 {background-image="Images/background-slate_v2.png" .center .smaller}

<br>

:::: {.columns}
::: {.column width="45%"}
CoW NMC Dataset

- `cinc_pct`
- `milex_billions`
- `milper_thousands`
- `irst_millions`
- `pec_thousand_tons`
- `tpop_millions`
- `upop_millions`
:::

::: {.column width="10%"}

:::

::: {.column width="45%"}
Fund for Peace Dataset

- `P3_Human_Rts`
:::
::::

::: notes

Your assignment is to pair each of these predictors from the NMC database one time with the P3 measure from the FFP to describe the association you see between them

- In other words, I want you to make seven scatterplots AND calculate the correlation for each NMC predictor paired with P3 human rights as the outcome

<br>

Notes to keep in mind:

1. Run these tests on just 2016

2. Your outcome variable ALWAYS goes on the y-axis in a scatterplot
    - e.g. P3 Human Rights is the outcome variation we are trying to explain
    
3. Correlations with missing data need extra arguments: use = "pairwise"

<br>

**Questions on the assignment?**

- Get to it!

- **OPTIONAL**: Next slide has results to discuss

<br>

<br>

NOTES FOR YOU: Not all of these will be required for the report.

- This exercise helps them practice their skills, test a bunch of hypotheses and will guide us in future steps (regression and confounders)

- for the report require 2 scatters (cinc and milper), plus... annotate a country?
:::








## {background-image="Images/background-slate_v2.png" .center .smaller}

```{r, echo=FALSE, fig.align='center', fig.asp=.618, fig.width=12}
d |>
  filter(Year == 2016) |>
  pivot_longer(cols = c(cinc_pct:upop_millions), names_to = "Predictor", values_to = "Value") |>
  ggplot(aes(x = Value, y = P3_Human_Rts)) +
  geom_point() +
  #geom_smooth(method = "lm") +
  theme_bw() +
  facet_wrap(~ Predictor, scales = "free_x", ncol = 4)
```

**Correlations with P3_Human_Rights**

```{r, echo=FALSE}
x1 <- d |>
  filter(Year == 2016) |>
  select(P3_Human_Rts, cinc_pct:upop_millions) |>
  cor(use = "pairwise")

round(x1[2:8,1], 2)
```

::: notes

Hoo boy, these results suck!

- But not shocking

- Can't explain variation with an absence of variation!

<br>

HOWEVER, all is not lost!

- Confounders matter (lessons to come)

- EVERYBODY filter the dataset for the current year (2016) AND let's focus on the developing world so also filter the bottom 50% of countries by cinc score
:::



## {background-image="Images/background-slate_v2.png" .center .smaller}

```{r, echo=FALSE, fig.align='center', fig.asp=.618, fig.width=12}
# What proportion are outliers here?
#summary(d$cinc_pct) # 3rd quartile is .366

# What would 95% of cases be?
#quantile(d$cinc_pct, probs = .95)

# Which states removed? 8 countries - Brazil, China, Germany, India, Japan, Russia, South Korea, United States
# d |>
#   filter(Year == 2016) |>
#   filter(cinc_pct > quantile(d$cinc_pct, probs = .95)) |> View()
#   select(Country) |>
#   arrange(Country)

d |>
  filter(Year == 2016) |>
  filter(cinc_pct < quantile(d$cinc_pct, probs = .5)) |>
  pivot_longer(cols = c(cinc_pct:upop_millions), names_to = "Predictor", values_to = "Value") |>
  ggplot(aes(x = Value, y = P3_Human_Rts)) +
  geom_point() +
  #geom_smooth(method = "lm", se=F) +
  theme_bw() +
  facet_wrap(~ Predictor, scales = "free_x", ncol = 4)
```

**Correlations with P3_Human_Rights**

```{r, echo=FALSE}
x1 <- d |>
  filter(Year == 2016) |>
  filter(cinc_pct < quantile(d$cinc_pct, probs = .5)) |>
  select(P3_Human_Rts, cinc_pct:upop_millions) |>
  cor(use = "pairwise")

round(x1[2:8,1], 2)
```

::: notes


:::





## Old Slides

## Rescaling Variables: log10 (Price) {background-image="Images/background-slate_v2.png" .center}

:::: {.columns}
::: {.column width="45%"}

<br>

```{r, echo=FALSE, fig.align='center', fig.asp=1, fig.width=6, cache=TRUE}
ggplot(diamonds, aes(x = price, y = forcats::fct_rev(cut))) +
  geom_boxplot(fill = "seagreen2") +
  theme_bw() +
  labs(x = "", y = "", title = "Does the quality of the a diamond's cut influence its price?", caption = "Source: The Loose Diamonds Search Engine") +
  scale_x_log10(labels = scales::dollar_format()) 
```
:::

::: {.column width="55%"}
```{r, echo=FALSE, fig.align='center', fig.asp=1.1, fig.width=7, cache=TRUE}
ggplot(diamonds, aes(x = price)) +
  geom_histogram(bins = 15) +
  facet_wrap(~ cut, ncol = 1) +
  theme_bw() +
  scale_x_log10(labels = scales::dollar_format())
```
:::
::::




## Rescaling Variables: The Log Scale {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center', fig.asp=.618, fig.width=9}
tibble(
  x1 = 1:10,
  x2 = log10(x1)
) |>
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Normal Scale", y = "Log 10 Scale",
       title = "The Common Logarithmic Scale (base 10)") +
  scale_x_continuous(breaks = 1:20) +
  scale_y_continuous(breaks = seq(0, 2, .1))
```

::: notes
While the most common transformations focus on moving a decimal place (e.g. making thousands into millions), there are a number of other options

<br>

One very useful tool is a family of transformations under the log scale

- On this plot I'm showing you how the numbers from 1-10 translate into a common log  scale (base 10).
:::



## Rescaling Variables: The Log Scale {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center', fig.asp=.618, fig.width=9}
tibble(
  x1 = 1:10,
  x2 = log10(x1)
) |>
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Normal Scale", y = "Log 10 Scale",
       title = "The Common Logarithmic Scale (base 10)") +
  scale_x_continuous(breaks = 1:20) +
  scale_y_continuous(breaks = seq(0, 2, .1)) +
  annotate("point", x = 4, y = .6, size = 4, color = "red") +
  annotate("point", x = 8, y = .9, size = 4, color = "red")
```

::: notes
For examples

- The log base 10 value of 5 is approximately .6, and

- The log 10 value of 6 is approximately .78
:::



## Rescaling Variables: The Log Scale {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center', fig.asp=.618, fig.width=9}
tibble(
  x1 = 1:10,
  x2 = log10(x1)
) |>
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Normal Scale", y = "Log 10 Scale",
       title = "The Common Logarithmic Scale (base 10)") +
  scale_x_continuous(breaks = 1:20) +
  scale_y_continuous(breaks = seq(0, 2, .1)) +
  annotate("point", x = 4, y = .6, size = 4, color = "red") +
  annotate("segment", x = 1, xend = 4, y = .6, yend = .6, linetype = "dashed", color = "red") +
  annotate("segment", x = 4, xend = 4, y = 0, yend = .6, linetype = "dashed", color = "red") +
  annotate("point", x = 8, y = .9, size = 4, color = "red") +
  annotate("segment", x = 1, xend = 8, y = .9, yend = .9, linetype = "dashed", color = "red") +
  annotate("segment", x = 8, xend = 8, y = 0, yend = .9, linetype = "dashed", color = "red")
```

::: notes
Moving from a value of 4 to 8 on our normal scale represents a difference of 4 units

<br>

BUT, on the log10 scale this 4 unit difference is reduced!

- The difference is now from .6 to .9 or .3

<br>

Moving to the log 10 scale makes differences on the normal scale much smaller.

<br>

**SLIDE**: The effect is even more stark as we get into much bigger numbers
:::



## Rescaling Variables: The Log Scale {background-image="Images/background-slate_v2.png" .center}

<br>

```{r, fig.align='center', fig.asp=.618, fig.width=9}
tibble(
  x1 = 1:10000,
  x2 = log10(x1)
) |>
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Normal Scale", y = "Log 10 Scale",
       title = "The Common Logarithmic Scale (base 10)") +
  #scale_x_continuous(breaks = 1:20) +
  #scale_y_continuous(breaks = seq(0, 2, .1)) +
  annotate("point", x = 5000, y = 3.7, size = 4, color = "red") +
  annotate("segment", x = 1, xend = 5000, y = 3.7, yend = 3.7, linetype = "dashed", color = "red") +
  annotate("segment", x = 5000, xend = 5000, y = 0, yend = 3.7, linetype = "dashed", color = "red") +
  annotate("point", x = 7500, y = 3.88, size = 4, color = "red") +
  annotate("segment", x = 1, xend = 7500, y = 3.88, yend = 3.88, linetype = "dashed", color = "red") +
  annotate("segment", x = 7500, xend = 7500, y = 0, yend = 3.88, linetype = "dashed", color = "red")
```

::: notes
Let's move into the thousands range and rexamine the log 10 scale

- A value of 5,000 on the normal scale corresponds to a log 10 value of approximately 3.7

- A value of 7,500, which is an increase of 2.5k, corresponds to a log 10 value of approximately 3.88

<br>

That means an increase of 2.5k is shrunk to only a difference of .18 on the log 10 scale!

<br>

**Anybody know why we might do this?**

- **What is the value of the log scale transformation?**

<br>

In short, the log scale helps us examine distributions where many observations are compressed in the smaller values OR where you have extreme outliers

- At the small end the log scale helps us distinguish between cases with similar values on the normal scale

- At the big end the log scale pulls outliers back towards the rest of the pack

<br>

**SLIDE**: Illustrate with GDP
:::


